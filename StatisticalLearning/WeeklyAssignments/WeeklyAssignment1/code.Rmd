---
title: "Assignnment 1"
output:
  pdf_document: default
  html_document: default
date: "2024-04-07"
---

```{r setup, include=FALSE, echo=FALSE, tidy=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(paran)
library(EFA.dimensions)
library(caret)
library(glmnet)
library(factoextra)
library(ggplot2)
library(knitr)
library(class)
library(FactoMineR)
library(qgraph)
library(stats)
library(tidyverse)
library(ggplot2)
```

# Question 1 

```{r, tidy=TRUE}
#load data
data_set <- read_csv("Data4168216.csv")


#split data into all data (all) and data without additional noise variables (set)
train_all <- data_set[1:5000, ]
test_all <- data_set[5001:10000, ]



train_all <- as.data.frame(train_all)
test_all <- as.data.frame(test_all)
set.seed(123)

train_set <- train_all[,1:7]
test_set <- test_all[, 1:7]
```


## 10-Fold Cross Validation for KNN, (limited predictors)
```{r, tidy=TRUE}
set.seed(123)

#split the total train data into 10 groups 
ctrl <- trainControl(method = "cv", number = 10)

#test values of k from 1 to 15 
k_vals <- 1:15
cv_results <- data.frame(k = k_vals, accuracy = numeric(length(k_vals)))

train_set$Y = as.factor(train_set$Y)

#for each value of k, create a model that is trained using k fold cross validation
# and calculate the accuracy of that model 
for (i in seq_along(k_vals)) {
  knn_model <- train(Y ~ X1+X2+X3+X4+X5+X6, data = train_set, 
                     method = "knn", trControl = ctrl, tuneGrid = data.frame(k = k_vals[i]))
  cv_results[i, "accuracy"] <- max(knn_model$results$Accuracy)
}

#find the optimal k, k with highest accuracy  
optimal_k <- cv_results$k[which.max(cv_results$accuracy)]
print(optimal_k)
```

## KNN with Optimal K (limited predictors)
```{r, tidy=TRUE}
predicted_classes <- knn(train_set[, -which(names(train_set) == "Y")], 
                         test_set[, -which(names(test_set) == "Y")], 
                         train_set$Y, k = optimal_k) 


accuracy <- mean(predicted_classes == test_set$Y)
print(accuracy)
```

## 10-Fold Cross Validation For Lambda (limited predictors)
```{r, tidy=TRUE}
set.seed(123)

x <- model.matrix(Y ~ X1+X2+X3+X4+X5+X6, data=train_set)
y <- train_set$Y 


cv_fit <- cv.glmnet(x = x, y = y, family = "binomial", nfolds = 10)

best_lambda_set <- cv_fit$lambda.min

best_lambda_set
```

## Lasso Regression (limited predictors) 
```{r, tidy=TRUE}

x_test <- model.matrix(Y ~ ., data = test_set)

lasso_model <- glmnet(x=x, y=y, family="binomial", alpha = 1, lambda = best_lambda_set)
preds<- predict(lasso_model, newx = x_test, type ="response" )

predicted_classes <- ifelse(preds > 0.5, 1, 0)

accuracy <- mean(predicted_classes == test_set$Y)

print(accuracy)

```


## 10-fold cross validation for KNN (all predictors) 
```{r, tidy=TRUE}

set.seed(123)

ctrl <- trainControl(method = "cv", number = 10)

k_vals <- 1:15
cv_results <- data.frame(k = k_vals, accuracy = numeric(length(k_vals)))

train_all$Y = as.factor(train_all$Y)

#10 fold cross validation for knn 
for (i in seq_along(k_vals)) {
  knn_model <- train(Y ~ ., data = train_all, method = "knn", trControl = ctrl, tuneGrid = data.frame(k = k_vals[i]))
  cv_results[i, "accuracy"] <- max(knn_model$results$Accuracy)
}

optimal_k <- cv_results$k[which.max(cv_results$accuracy)]
print(optimal_k)
```


## KNN With Optimal K (all predictors) 
```{r, tidy=TRUE}

set.seed(123)
predicted_classes <- knn(train_all[, -which(names(train_all) == "Y")], 
                         test_all[, -which(names(test_all) == "Y")], 
                         train_all$Y, k = optimal_k) 

accuracy_knn_all <- mean(predicted_classes == test_all$Y)
print(accuracy_knn_all)
```

## 1O fold Cross Validation for Lambda (all predictors) 

```{r, tidy=TRUE}

set.seed(123)

x <- model.matrix(Y ~., data=train_all)
y <- train_all$Y 


cv_fit <- cv.glmnet(x = x, y = y, family = "binomial", nfolds = 10)

best_lambda_all <- cv_fit$lambda.min

best_lambda_all

```


## Lasso Regression (all predictors)
```{r, tidy=TRUE}

lasso_model <- glmnet(x=x, y=y, family="binomial", alpha = 1, lambda = best_lambda_all )

x_test_all <- model.matrix(Y ~ ., data = test_all)
preds<- predict(lasso_model, newx = x_test_all, type ="response" )

predicted_classes <- ifelse(preds > 0.5, 1, 0)

accuracy_lr_all <- mean(predicted_classes == test_set$Y)

print(accuracy_lr_all)

```

# Question 2 


## Kaiser's Rule 

```{r, tidy=TRUE}
data_set <- read_csv("data.US.csv")
data_set = subset(data_set, select = -c(...1) )
pca_result <- prcomp(data_set, scale=TRUE)
plot(pca_result, xlab="Principal Components", main="Principle Component Analysis" )
abline(h = 1, col = "blue", lty = 1) #eigenvalues should be > 1 

```

Since Kaiser rule states that only the principal components whose eigenvalues exceed 1 should be retained, the first 5 components are selected for PCA. the eigenvalue has to be more than 1, the first 5 components should be selected from PCA.


## Scree Plots and Elbow Rule 


```{r screeplot, echo=FALSE, tidy=TRUE}

par(mfrow=c(2,2))
prop_var_expl_sc = summary(pca_result)$importance[2,]
cum_var_expl_sc = summary(pca_result)$importance[3,]

plot(prop_var_expl_sc, type="b", xlab="Component number", ylab="Proportion of variance explained", ylim=c(0,1))
plot(cum_var_expl_sc, type="b", xlab="Number of components", ylab="Cumulative variance explained", ylim=c(0,1))
```
## Velicer's MAP test 

```{r, velicer, tidy=TRUE}

MAP(data_set, corkind='pearson', verbose = TRUE)
```
## Horn's Parallel Analysis

```{r, hornspar, tidy=TRUE}
parallel_result <- paran(data_set)
```
## Contributions of Each Variable to PCs

```{r, tidy=TRUE}

data_set <- read_csv("data.US.csv")

data.pca <- prcomp(data_set, scale = TRUE, center = TRUE)
var <- get_pca_var(data.pca)

# Contributions of variables to PC1
fviz_contrib(data.pca, choice = "var", axes = 1, top = 10) + xlab("Variables from V1 onwards (truncated)") + theme(axis.title.x = element_text(size = 14),axis.title.y = element_text(size = 14))
fviz_contrib(data.pca, choice = "var", axes = 2, top = 10) + xlab("Variables from V1 onwards (truncated)") + theme(axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14))
fviz_contrib(data.pca, choice = "var", axes = 3, top = 10) + xlab("Variables from V1 onwards (truncated)") + theme(axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14))
fviz_contrib(data.pca, choice = "var", axes = 4, top = 10) + xlab("Variables from V1 onwards (truncated)") + theme(axis.title.x = element_text(size = 14),axis.title.y = element_text(size = 14))
fviz_contrib(data.pca, choice = "var", axes = 5, top = 10) + xlab("Variables from V1 onwards (truncated)") + theme(axis.title.x = element_text(size = 14),
axis.title.y = element_text(size = 14))

```
```{r, tidy=TRUE}
summary(pca_result)
```


## K Means Clustering 

```{r, tidy=TRUE}

pca_data <- as.data.frame(pca_result$rotation[,1:5]) 

fviz_nbclust(x = pca_data ,FUNcluster = kmeans, method = 'wss' )
```



```{r,tidy=TRUE}
k = 5
km.res <- kmeans(pca_data, centers=k, nstart = 25)

#print(km.res)

fviz_cluster(km.res, data=pca_data)

```
## Hierarchical Clustering

```{r, tidy=TRUE}
res.hcpc <- HCPC(pca_data, graph = FALSE)
fviz_dend(res.hcpc, 
          cex = 0.7,                     # Label size
          palette = "jco",               # Color palette see ?ggpubr::ggpar
          rect = TRUE, rect_fill = TRUE, # Add rectangle around groups
          rect_border = "jco",           # Rectangle color
          labels_track_height = 0.8   # Augment the room for label
          )


```






